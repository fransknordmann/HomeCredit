{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist - P7 - Laurent Trichet\n",
    "\n",
    "## Implémentez un modèle de scoring\n",
    "\n",
    "## TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import default libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Garbage Collector (empty dataFrame memory)\n",
    "import gc\n",
    "\n",
    "# Remove some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "\n",
    "# Import Imbalanced-learn necessary tools\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "\n",
    "# Import for classification GradientBoostingClassifier & SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "# Import for classification xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import evaluation tool for classification optimisations\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Imports tools for model interpreation, AUC, roc, permutations\n",
    "from sklearn import metrics\n",
    "\n",
    "# tools for execution time estimates\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Pandas parameters\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "pd.set_option('display.max_info_rows', 2000)\n",
    "\n",
    "# Matplotlib and sns visual parameters\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 11\n",
    "mpl.rcParams['ytick.labelsize'] = 11\n",
    "\n",
    "# Constants\n",
    "DIRSOURCE = '../Sources/'\n",
    "DIRDATASET = '../credithome_datasets/'\n",
    "NUMROWS = 15000    # 1000000 to get complete dateset\n",
    "# File names with NUMROWS lines and Fill nan with zeros\n",
    "FILESTD_FNAN0_REDUCED = DIRDATASET+'Credit_Home_Junction_Std_Fnan0_Reduced_'+str(NUMROWS)+'.csv'\n",
    "FILEFEAT_IN = DIRDATASET+'Credit_Home_Features.csv'\n",
    "FILEFILT_IN = DIRDATASET+'Credit_Home_Filters.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data sets & list features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILESTD_FNAN0_REDUCED, encoding='Latin-1', sep='\\t')\n",
    "df_filter = pd.read_csv(FILEFILT_IN, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.read_csv(FILEFEAT_IN, sep='\\t')\n",
    "# drop columns of features with importance == 0\n",
    "df_feat = df_feat.iloc[0:12,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of web API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API Web\n",
    "api_url = 'http://oc.14eight.com:5001/api/'\n",
    "#api_url = 'http://ec2-18-218-21-153.us-east-2.compute.amazonaws.com:5001/api/'\n",
    "# api_url = 'http://localhost:5001/api/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [[]]}\n",
      "V\n",
      "http://oc.14eight.com:5001/api/client_list/\n",
      "V\n",
      "100983 - Woman - 46 years old - Amount Loan: USD $625536\n",
      "100984 - Man - 30 years old - Amount Loan: USD $675000\n",
      "100987 - Woman - 33 years old - Amount Loan: USD $500490\n",
      "100997 - Man - 29 years old - Amount Loan: USD $199152\n",
      "100998 - Woman - 31 years old - Amount Loan: USD $601470\n",
      "101002 - Woman - 54 years old - Amount Loan: USD $257391\n",
      "101005 - Woman - 30 years old - Amount Loan: USD $900000\n",
      "101008 - Woman - 36 years old - Amount Loan: USD $497520\n",
      "101020 - Man - 46 years old - Amount Loan: USD $332946\n",
      "101021 - Man - 48 years old - Amount Loan: USD $225000\n",
      "101024 - Woman - 34 years old - Amount Loan: USD $582804\n",
      "101025 - Woman - 31 years old - Amount Loan: USD $375408\n",
      "101026 - Man - 60 years old - Amount Loan: USD $450000\n",
      "101031 - Woman - 58 years old - Amount Loan: USD $174132\n",
      "101041 - Man - 49 years old - Amount Loan: USD $454500\n",
      "101051 - Man - 45 years old - Amount Loan: USD $400392\n",
      "101055 - Man - 40 years old - Amount Loan: USD $157500\n",
      "101056 - Man - 27 years old - Amount Loan: USD $358344\n",
      "101057 - Woman - 29 years old - Amount Loan: USD $202500\n",
      "101064 - Man - 61 years old - Amount Loan: USD $112500\n",
      "101079 - Man - 56 years old - Amount Loan: USD $547344\n"
     ]
    }
   ],
   "source": [
    "api_test = 'client_list/'\n",
    "\n",
    "data = []\n",
    "data_json = {'data': [data]}\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0,len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SK_ID_CURR': 100983}\n",
      "V\n",
      "http://oc.14eight.com:5001/api/client_score/\n",
      "V\n",
      "HTTP error: 500\n"
     ]
    }
   ],
   "source": [
    "api_test = 'client_score/'\n",
    "data_id_client = 100983\n",
    "data_json = {\n",
    "    'SK_ID_CURR': data_id_client,\n",
    "    }\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    score = data_json['score']\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [[]]}\n",
      "V\n",
      "http://oc.14eight.com:5001/api/feature_list/\n",
      "V\n",
      "['EXT_SOURCE_3', 0.1, '(Application) Normalized score from external data source']\n",
      "['EXT_SOURCE_2', 0.07, '(Application) Normalized score from external data source']\n",
      "['AMT_ANNUITY', 0.02, '(Application) Loan annuity']\n",
      "['EXT_SOURCE_1', 0.02, '(Application) Normalized score from external data source']\n",
      "['DAYS_BIRTH', 0.02, \"(Application) Client's age at the time of application\"]\n",
      "['CODE_GENDER', 0.01, '(Application) Gender of the client']\n",
      "['AMT_GOODS_PRICE', 0.01, '(Application) For consumer loans it is the price of the goods for which the loan is given']\n",
      "['BURO_DAYS_CREDIT_MAX', 0.01, '(Bureau) MAX, How many days before current application did client apply for Credit Bureau credit']\n",
      "['INSTAL_DAYS_ENTRY_PAYMENT_MAX', 0.01, '(Installments Payments) MAX, When was the installments of previous credit paid actually (relative to application date of current loan)']\n",
      "['DAYS_ID_PUBLISH', 0.01, '(Application) How many days before the application did client change the identity document with which he applied for the loan']\n",
      "['BURO_DAYS_CREDIT_MEAN', 0.01, '(Bureau) MEAN, How many days before current application did client apply for Credit Bureau credit']\n",
      "['DAYS_LAST_PHONE_CHANGE', 0.01, '(Application) How many days before application did client change phone']\n"
     ]
    }
   ],
   "source": [
    "api_test = 'feature_list/'\n",
    "\n",
    "data = []\n",
    "data_json = {'data': [data]}\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0,len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_test = 'fnr_list/'\n",
    "\n",
    "data = []\n",
    "data_json = {'data': [data]}\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0,len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_test = 'fpr_list/'\n",
    "\n",
    "data = []\n",
    "data_json = {'data': [data]}\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0,len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list(inlist):\n",
    "    outlist = []\n",
    "    for i in np.arange(len(inlist)):\n",
    "        if inlist[i][0] in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "            outlist.append(inlist[i])\n",
    "    return outlist\n",
    "\n",
    "\n",
    "risk_fpr_list = data\n",
    "risk_fpr_list = filter_list(risk_fpr_list)\n",
    "\n",
    "risk_marks ={0:'100%'}\n",
    "for i in np.arange(0, len(risk_fpr_list)):\n",
    "    percent = f'max {int(100*risk_fpr_list[i][0])}%'\n",
    "    mark = 100*risk_fpr_list[i][1]\n",
    "    risk_marks[int(mark)] = percent\n",
    "risk_marks[100] = '0%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_fpr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "score = 0.78\n",
    "i=0\n",
    "while i < len(risk_fpr_list):\n",
    "    if score > risk_fpr_list[i][1]:\n",
    "        break\n",
    "    i = i + 1\n",
    "librisk = int(100 * risk_fpr_list[i][1])\n",
    "score = int(100 * score)\n",
    "n = len(risk_fpr_list)\n",
    "fig = go.Figure(\n",
    "        go.Indicator(\n",
    "            domain={'x': [0, 1], 'y': [0, 1]},\n",
    "            value=score,\n",
    "            mode=\"gauge+number+delta\",\n",
    "            title={'text': \"Score\"},\n",
    "            # delta={'reference': 70},\n",
    "            gauge={'axis': {'range': [None, 100]},\n",
    "                'steps': [\n",
    "                    {'range': [100*risk_fpr_list[0][1], 100],\n",
    "                     'color': \"#FFDFDF\"},\n",
    "                    {'range': [100*risk_fpr_list[1][1], 100*risk_fpr_list[0][1]],\n",
    "                     'color': \"#FFBFBF\"},\n",
    "                    {'range': [100*risk_fpr_list[2][1], 100*risk_fpr_list[1][1]],\n",
    "                     'color': \"#FF9F9F\"},\n",
    "                    {'range': [100*risk_fpr_list[3][1], 100*risk_fpr_list[2][1]],\n",
    "                     'color': \"#FF7F7F\"},\n",
    "                    {'range': [100*risk_fpr_list[4][1], 100*risk_fpr_list[3][1]],\n",
    "                     'color': \"#FF5F5F\"},\n",
    "                    {'range': [100*risk_fpr_list[5][1], 100*risk_fpr_list[4][1]],\n",
    "                     'color': \"#FF3F3F\"},\n",
    "                    {'range': [100*risk_fpr_list[6][1], 100*risk_fpr_list[5][1]],\n",
    "                     'color': \"#FF1F1F\"},\n",
    "                    {'range': [0, 100*risk_fpr_list[6][1]],\n",
    "                     'color': \"#FF0F0F\"},\n",
    "                        ],\n",
    "           'threshold': {'line': {'color': \"green\", 'width': 4},\n",
    "                         'thickness': 0.75, 'value': librisk}\n",
    "                    }\n",
    "        )\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SK_ID_CURR': 100001, 'FILTER_Y_N': 'Y', 'RANGES': [[0.6, 1], [0, 1], [0, 999999], [0, 1], [-99, -60], [0, 1]]}\n",
      "V\n",
      "http://localhost:5001/api/feature_values/\n",
      "V\n",
      "[[0.16, 0.79, 20560.5, 0.75, -53.0, 1.0, 450000.0, -49.0, -1628.0, -812.0, -735.0, -1740.0]]\n",
      "[22, 803]\n",
      "['EXT_SOURCE_3', 0.16, 0.64, 0.72, 0.83, 0.6, 0.71, 0.88]\n",
      "['EXT_SOURCE_2', 0.79, 0.01, 0.38, 0.76, 0.0, 0.53, 0.8]\n",
      "['AMT_ANNUITY', 20560.5, 9265.5, 23404.7, 58203.0, 2596.5, 23891.45, 110488.5]\n",
      "['EXT_SOURCE_1', 0.75, 0.0, 0.15, 0.82, 0.0, 0.15, 0.93]\n",
      "['DAYS_BIRTH', -53.0, -69.0, -63.59, -60.0, -69.0, -63.53, -60.0]\n",
      "['CODE_GENDER', 1.0, 0.0, 0.68, 1.0, 0.0, 0.79, 1.0]\n"
     ]
    }
   ],
   "source": [
    "api_test = 'feature_values/'\n",
    "data_id_client = 100001\n",
    "data_filter = 'Y'\n",
    "data = [[0.0, 1], [0, 1], [0, 999999], [0, 1], [-99, -60], [0, 1]]\n",
    "data_json = {\n",
    "    'SK_ID_CURR': data_id_client,\n",
    "    'FILTER_Y_N': data_filter,\n",
    "    'RANGES': data,\n",
    "    }\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data_client = data_json['data_client']\n",
    "    print(data_client)\n",
    "    pop = data_json['population']\n",
    "    print(pop)\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0,len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SK_ID_CURR': 100001, 'FILTER_Y_N': 'N', 'RANGES': [[0, 1], [0, 1], [0, 999999], [0, 1], [-99, 0]]}\n",
      "V\n",
      "http://oc.14eight.com:5001/api/feature_values/\n",
      "V\n",
      "[[0.16, 0.79, 20560.5, 0.75, -53.0, 1.0, 450000.0, -49.0, -1628.0, -812.0, -735.0, -1740.0]]\n",
      "[1174, 13826]\n",
      "['EXT_SOURCE_3', 0.16, 0.0, 0.29, 0.86, 0.0, 0.42, 0.89]\n",
      "['EXT_SOURCE_2', 0.79, 0.0, 0.42, 0.8, 0.0, 0.52, 0.85]\n",
      "['AMT_ANNUITY', 20560.5, 2844.0, 26742.22, 105511.5, 2596.5, 27133.93, 225000.0]\n",
      "['EXT_SOURCE_1', 0.75, 0.0, 0.16, 0.93, 0.0, 0.22, 0.93]\n",
      "['DAYS_BIRTH', -53.0, -69.0, -40.68, -22.0, -69.0, -44.63, -22.0]\n",
      "['CODE_GENDER', 1.0, 0.0, 0.57, 1.0, 0.0, 0.66, 1.0]\n",
      "['AMT_GOODS_PRICE', 450000.0, 0.0, 488310.2, 2961000.0, 0.0, 544903.73, 4050000.0]\n",
      "['BURO_DAYS_CREDIT_MAX', -49.0, -2922.0, -308.86, 0.0, -2922.0, -433.87, 0.0]\n",
      "['INSTAL_DAYS_ENTRY_PAYMENT_MAX', -1628.0, -2931.0, -344.77, 0.0, -2920.0, -308.18, 0.0]\n",
      "['DAYS_ID_PUBLISH', -812.0, -5888.0, -2684.38, -10.0, -6228.0, -3000.77, 0.0]\n",
      "['BURO_DAYS_CREDIT_MEAN', -735.0, -2922.0, -716.1, 0.0, -2922.0, -949.49, 0.0]\n",
      "['DAYS_LAST_PHONE_CHANGE', -1740.0, -3235.0, -779.07, 0.0, -3983.0, -967.45, 0.0]\n"
     ]
    }
   ],
   "source": [
    "api_test = 'feature_values/'\n",
    "data_id_client = 100001\n",
    "data_filter = 'N'\n",
    "data = [[0, 1], [0, 1], [0, 999999], [0, 1], [-99, 0]]\n",
    "data_json = {\n",
    "    'SK_ID_CURR': data_id_client,\n",
    "    'FILTER_Y_N': data_filter,\n",
    "    'RANGES': data,\n",
    "    }\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data_client = data_json['data_client']\n",
    "    print(data_client)\n",
    "    pop = data_json['population']\n",
    "    print(pop)\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0,len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_test = 'population_count/'\n",
    "data_filter = 'Y'\n",
    "data = [[0.6, 1], [0, 1], [0, 999999], [0, 1], [-99, -60], [0, 1]]\n",
    "data_json = {\n",
    "    'FILTER_Y_N': data_filter,\n",
    "    'RANGES': data,\n",
    "    }\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    pop = data_json['population']\n",
    "    print(pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_test = 'filters/'\n",
    "data = []\n",
    "data_json = {'data': [data]}\n",
    "print(data_json)\n",
    "print('V')\n",
    "print(api_url+api_test)\n",
    "print('V')\n",
    "response = requests.request(method='POST',\n",
    "                            headers=headers,\n",
    "                            url=api_url+api_test,\n",
    "                            json=data_json\n",
    "                        )\n",
    "if response.status_code != 200:\n",
    "    print(f'HTTP error: {response.status_code}')\n",
    "else:\n",
    "    data_json = response.json()\n",
    "    data = data_json['data']\n",
    "    for i in np.arange(0, len(data)):\n",
    "        print(data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_features = [c for c in df.columns if c not in ['index', 'TARGET', 'SK_ID_CURR']]\n",
    "df_test = df[df['TARGET']==999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API AWS OR LAPTOP\n",
    "model_url = 'http://oc.14eight.com:5000/invocations'\n",
    "# model_url = 'http://ec2-18-218-21-153.us-east-2.compute.amazonaws.com:5000/invocations'\n",
    "# model_url = 'http://localhost:5000/invocations'\n",
    "\n",
    "max_val = 20\n",
    "tab_rep = np.zeros(max_val)\n",
    "tab_id = []\n",
    "first_pos = np.random.randint(1,df_test.shape[0]-max_val)\n",
    "for i in np.arange(0, max_val):\n",
    "    data = df_test[c_features].iloc[first_pos+i,:].to_list()\n",
    "    data_json = {'data': [data]}\n",
    "    response = requests.request(method='POST',\n",
    "                                headers=headers,\n",
    "                                url=model_url,\n",
    "                                json=data_json\n",
    "                            )\n",
    "    if response.status_code != 200:\n",
    "        print(f'HTTP error: {response.status_code}')\n",
    "    else:\n",
    "        tab_id.append([first_pos+i, df_test.iloc[first_pos+i,:]['SK_ID_CURR']])\n",
    "        tab_rep[i] = response.json()[0]\n",
    "        print(f'{i:3} / {max_val-1:3} {response.json()}        ', end='\\r')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_names = df_feat[df_feat['mean importance']==0]['col name'].values\n",
    "df.drop(columns=col_names, inplace=True)\n",
    "\n",
    "# Retrieve train and test datasets\n",
    "df_train = df[df['TARGET']!=999]\n",
    "df_test = df[df['TARGET']==999]\n",
    "# Keep valid columns for features and result class in future classifications\n",
    "c_features = [c for c in df.columns if c not in ['index', 'TARGET', 'SK_ID_CURR']]\n",
    "c_class = 'TARGET'\n",
    "\n",
    "del df, df_feat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix imbalanced data with Prototype selection (under sample of positive class included in original sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter1 = Counter(df_train[c_class])\n",
    "print(counter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = imblearn.under_sampling.RandomUnderSampler(random_state=0)\n",
    "X, y = undersample.fit_resample(df_train[c_features], df_train[c_class])\n",
    "\n",
    "counter2 = Counter(y)\n",
    "print(counter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2,\n",
    "                        sharex=False, sharey=False,\n",
    "                        figsize=(16,5))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for label, _ in counter1.items():\n",
    "    row_ix = np.where(df_train[c_class].values == label)[0]\n",
    "    sns.scatterplot(df_train[c_features].iloc[row_ix, 8],\n",
    "                    df_train[c_features].iloc[row_ix, 9],\n",
    "                    label=str(label),\n",
    "                    ax=axes[0]\n",
    "                    )\n",
    "axes[0].set_title('Imbalanced data')\n",
    "\n",
    "for label, _ in counter2.items():\n",
    "    row_ix = np.where(y.values == label)[0]\n",
    "    sns.scatterplot(X.iloc[row_ix, 8],\n",
    "                    X.iloc[row_ix, 9],\n",
    "                    label=str(label),\n",
    "                    ax=axes[1]\n",
    "                    )\n",
    "axes[1].set_title('Random Under Sample')\n",
    "print('\\n\\tArbitrary selection of 2 variables to see effect of under sampling ...')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Search for Classification method & Hyperparameters. WE VERIFY THAT WE GAIN 30% OF TIME OF TREATMENT WITH REDUCED SET  OF FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC, XGBCClassifier, GradientBoostingClassifier best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "iname, itype, iparam = 0, 1, 2\n",
    "models.append(['LinearSVC ', svm.LinearSVC(),\n",
    "               { \n",
    "                'C': np.logspace(-4, 4, 9),\n",
    "                'penalty' : ['l1', 'l2'],\n",
    "                'loss': ['hinge', 'squared_hinge'],\n",
    "                'dual': [False],\n",
    "               }\n",
    "              ])\n",
    "models.append(['XGBClassifier', XGBClassifier(),\n",
    "               {\n",
    "                 'max_depth': [3,5],\n",
    "                 'min_child_weight': [1, 5, 10],\n",
    "                 'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "                 'subsample': [0.6, 0.8, 1.0],\n",
    "                 'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                 'verbosity': [0],\n",
    "               }\n",
    "              ])\n",
    "models.append(['GradBoostC', ensemble.GradientBoostingClassifier(),\n",
    "               {\n",
    "                'n_estimators': [200],\n",
    "                'max_depth': [3,5],\n",
    "                'criterion': ['friedman_mse', 'squared_error'],\n",
    "                'min_samples_split': [2, 3, 4],\n",
    "                'min_weight_fraction_leaf': [0.0, 0.2, 0.4],\n",
    "               }\n",
    "              ])\n",
    "for i, model in enumerate(models):\n",
    "    mdl = GridSearchCV(model[itype], model[iparam], cv=5, scoring='roc_auc')\n",
    "    datedeb = datetime.now()\n",
    "    mdl.fit(X, y)\n",
    "    duree = datetime.now() - datedeb\n",
    "    print(f'{model[iname]} \\tduree: {duree.seconds}s \\tbest_score: {mdl.best_score_:4.3} \\tbest_params: {mdl.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For a 1174 '1 and 0 balanced classes' sample WITH ALL ORIGINAL FEATURES:  \n",
    "  \n",
    ">> LinearSVC  \tduree: 112s \tbest_score: 0.696 \tbest_params: {'C': 0.1, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l1'}  \n",
    ">>  \n",
    ">>  XGBClassifier \tduree: 1663s \tbest_score: 0.723 \tbest_params: {'colsample_bytree': 1.0, 'gamma': 1.5, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 1.0, 'verbosity': 0}  \n",
    ">>  \n",
    ">>  GradBoostC \tduree: 1631s \tbest_score: 0.732 \tbest_params: {'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.2, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Kfold Roc Curve and Feature Importances, WE VERIFY THAT WE HAVE THE SAME RESULTS AS WITH ALL ORIGINAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "tot_valid_y = np.zeros(y.shape[0])\n",
    "tot_valid_prob = np.zeros(y.shape[0])\n",
    "tot_score = []\n",
    "tot_feature_importances = []\n",
    "\n",
    "for splt, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    train_x, train_y = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    valid_x, valid_y = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # GradientBoostingClassifier\n",
    "    gbc = ensemble.GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        criterion='friedman_mse',\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        min_weight_fraction_leaf=0.2,\n",
    "    )\n",
    "    gbc.fit(train_x, train_y)\n",
    "\n",
    "    tot_valid_y[valid_idx] = valid_y\n",
    "    \n",
    "    valid_prob = gbc.predict_proba(valid_x)[:,1]\n",
    "    tot_valid_prob[valid_idx] = valid_prob\n",
    "    \n",
    "    tot_score.append(metrics.roc_auc_score(valid_y.values, valid_prob))\n",
    "    tot_feature_importances.append(gbc.feature_importances_)\n",
    "    \n",
    "tot_score = [round(1000*s)/1000 for s in tot_score] \n",
    "mean_score = sum(tot_score)/len(tot_score)\n",
    "print(f'tot_score   = {[s for s in tot_score]}')\n",
    "print(f'mean scores = {mean_score:5.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axe = plt.subplots(figsize=(10,7))\n",
    "[fpr, tpr, thr] = metrics.roc_curve(tot_valid_y,\n",
    "                                    tot_valid_prob,\n",
    "                                    pos_label=1)\n",
    "axe.plot(fpr, tpr, color='orange', lw=2)\n",
    "axe.set_title(f'Roc curve ({n_splits} splits) mean AUC = {mean_score:5.3}')\n",
    "axe.set_xlabel('Specificity')\n",
    "axe.set_ylabel('Sensitivity')\n",
    "axe.grid(visible=True, color='#eeeeee')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Threshold 1: MINIMIZE THE RISK : we want to minimize the rate of False Positive, i.e minimize the pourcentage of loans attributed to wrong clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display thresholds for different False Positive Rate (FPR) tolerances\n",
    "for max_fpr in np.arange(0.05, 0.65, 0.05):\n",
    "    idx = np.max(np.where(fpr<max_fpr))\n",
    "    str1 = f'False Pos Rate max {max_fpr:.2f} '\n",
    "    str2 = f'Sensitivity: {tpr[idx]:.2f},  Specificity: {1-fpr[idx]:.2f} Threshold: {thr[idx]:.2f}'\n",
    "    print(str1+str2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Threshold 2: OPTIMIZE THE TURNOVER : we want to minimize the rate of False Negative, i.e minimize the pourcentage of loans refused to good clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display thresholds for different False Negative Rate (FNR) tolerances\n",
    "# FNR = 1 - TPR\n",
    "for min_tpr in np.arange(0.40, 1, 0.05):\n",
    "    idx = np.min(np.where(tpr>min_tpr))\n",
    "    str1 = f'False Neg Rate max {1-min_tpr:.2f} '\n",
    "    str2 = f'Sensitivity: {tpr[idx]:.2f},  Specificity: {1-fpr[idx]:.2f} Threshold: {thr[idx]:.2f}'\n",
    "    print(str1+str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape features and importances to find features with main role in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_mean = pd.DataFrame(tot_feature_importances).mean().to_list()\n",
    "importance_std = pd.DataFrame(tot_feature_importances).std().to_list()\n",
    "df_features = pd.DataFrame(data=np.array([[c for c in X.columns], importance_mean, importance_std]).T,\n",
    "                           columns=['col name', 'mean def', 'std def'])\n",
    "df_features['mean def'] = df_features['mean def'].astype('float64')\n",
    "df_features['std def'] = df_features['std def'].astype('float64')\n",
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_draw = df_features.sort_values('mean def')\n",
    "df_draw = df_draw.iloc[-80:,:]\n",
    "fig, axes = plt.subplots(figsize=(14,int(df_draw.shape[0]//3.5)))\n",
    "axes.barh([x for x in range(df_draw.shape[0])],\n",
    "           df_draw['mean def'].values,\n",
    "           xerr = df_draw['std def'].values,\n",
    "           color = '#33aa33',\n",
    "           tick_label=df_draw['col name'].values)\n",
    "axes.set_title(f'Features Importance KFOLD - {df_draw.shape[0]} first features')\n",
    "axes.grid(visible=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Save Credit Home Junction File with reduced set of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMROWS = 1000000    # 1000000 to get complete dateset\n",
    "# File names with NUMROWS lines and Fill nan with zeros\n",
    "FILESTD_FNAN0 = DIRDATASET+'Credit_Home_Junction_Std_Fnan0_'+str(NUMROWS)+'.csv'\n",
    "FILESTD_FNAN0_REDUCED = DIRDATASET+'Credit_Home_Junction_Std_Fnan0_Reduced_'+str(NUMROWS)+'.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILESTD_FNAN0, encoding='Latin-1', sep='\\t')\n",
    "\n",
    "# drop columns of features with importance == 0\n",
    "df_feat = pd.read_csv(FILEFEAT_IN, sep='\\t')\n",
    "col_names = df_feat[df_feat['mean importance']==0]['col name'].values\n",
    "df.drop(columns=col_names, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(FILESTD_FNAN0_REDUCED, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "438838f77353e196c1617684cb98b864bf1da2ed28cf8470ba0df88c56da92f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
