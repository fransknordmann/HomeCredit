{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist - P7 - Laurent Trichet\n",
    "\n",
    "## Implémentez un modèle de scoring\n",
    "\n",
    "## 3 Classification (with conversion to log for some data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries  - Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import default libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Garbage Collector (empty dataFrame memory)\n",
    "import gc\n",
    "\n",
    "# Remove some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "\n",
    "# Import Imbalanced-learn necessary tools\n",
    "import imblearn\n",
    "from collections import Counter\n",
    "\n",
    "# Import for classification GradientBoostingClassifier & SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "# Import for classification xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import evaluation tool for classification optimisations\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# Imports tools for model interpreation, AUC, roc, permutations\n",
    "from sklearn import metrics\n",
    "from sklearn import inspection\n",
    "\n",
    "# tools for execution time estimates\n",
    "from datetime import datetime\n",
    "\n",
    "# Pandas parameters\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 120)\n",
    "pd.set_option('display.max_info_rows', 2000)\n",
    "\n",
    "# Matplotlib and sns visual parameters\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('paper')\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['xtick.labelsize'] = 11\n",
    "mpl.rcParams['ytick.labelsize'] = 11\n",
    "\n",
    "# Constants\n",
    "DIRSOURCE = '../Sources/'\n",
    "DIRDATASET = './credithome_datasets/'\n",
    "NUMROWS = 15000    # 1000000 to get complete dateset\n",
    "# File names with NUMROWS lines and Fill nan with zeros\n",
    "FILESTD_FNAN0 = DIRDATASET+'Credit_Home_Junction_Std_Fnan0_'+str(NUMROWS)+'.csv'\n",
    "FILELOG_FNAN0 = DIRDATASET+'Credit_Home_Junction_Log_Fnan0_'+str(NUMROWS)+'.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load training and test sets, apply correction of imbalanced classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read reduced dataset (15000) and prepare training and test features and result class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILELOG_FNAN0, encoding='Latin-1', sep='\\t')\n",
    "\n",
    "# Retrieve train and test datasets\n",
    "df_train = df[df['TARGET']!=999]\n",
    "df_test = df[df['TARGET']==999]\n",
    "# Keep valid columns for features and result class in future classifications\n",
    "c_features = [c for c in df.columns if c not in ['index', 'TARGET', 'SK_ID_CURR']]\n",
    "c_class = 'TARGET'\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix imbalanced data with Prototype selection (under sample of positive class included in original sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter1 = Counter(df_train[c_class])\n",
    "print(counter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = imblearn.under_sampling.RandomUnderSampler(random_state=0)\n",
    "X, y = undersample.fit_resample(df_train[c_features], df_train[c_class])\n",
    "\n",
    "counter2 = Counter(y)\n",
    "print(counter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2,\n",
    "                        sharex=False, sharey=False,\n",
    "                        figsize=(16,5))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "for label, _ in counter1.items():\n",
    "    row_ix = np.where(df_train[c_class].values == label)[0]\n",
    "    sns.scatterplot(df_train[c_features].iloc[row_ix, 10],\n",
    "                    df_train[c_features].iloc[row_ix, 11],\n",
    "                    label=str(label),\n",
    "                    ax=axes[0]\n",
    "                    )\n",
    "axes[0].set_title('Imbalanced data')\n",
    "\n",
    "for label, _ in counter2.items():\n",
    "    row_ix = np.where(y.values == label)[0]\n",
    "    sns.scatterplot(X.iloc[row_ix, 10],\n",
    "                    X.iloc[row_ix, 11],\n",
    "                    label=str(label),\n",
    "                    ax=axes[1]\n",
    "                    )\n",
    "axes[1].set_title('Random Under Sample')\n",
    "print('\\n\\tArbitrary selection of 2 variables to see effect of under sampling ...')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Search for Classification method & Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC, XGBCClassifier, GradientBoostingClassifier best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "iname, itype, iparam = 0, 1, 2\n",
    "models.append(['LinearSVC ', svm.LinearSVC(),\n",
    "               { \n",
    "                'C': np.logspace(-4, 4, 9),\n",
    "                'penalty' : ['l1', 'l2'],\n",
    "                'loss': ['hinge', 'squared_hinge'],\n",
    "                'dual': [False],\n",
    "               }\n",
    "              ])\n",
    "models.append(['XGBClassifier', XGBClassifier(),\n",
    "               {\n",
    "                 'max_depth': [3,5],\n",
    "                 'min_child_weight': [1, 5, 10],\n",
    "                 'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "                 'subsample': [0.6, 0.8, 1.0],\n",
    "                 'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "                 'verbosity': [0],\n",
    "               }\n",
    "              ])\n",
    "models.append(['GradBoostC', ensemble.GradientBoostingClassifier(),\n",
    "               {\n",
    "                'n_estimators': [200],\n",
    "                'max_depth': [3,5],\n",
    "                'criterion': ['friedman_mse', 'squared_error'],\n",
    "                'min_samples_split': [2, 3, 4],\n",
    "                'min_weight_fraction_leaf': [0.0, 0.2, 0.4],\n",
    "               }\n",
    "              ])\n",
    "for i, model in enumerate(models):\n",
    "    mdl = GridSearchCV(model[itype], model[iparam], cv=5, scoring='roc_auc')\n",
    "    datedeb = datetime.now()\n",
    "    mdl.fit(X, y)\n",
    "    duree = datetime.now() - datedeb\n",
    "    print(f'{model[iname]} \\tduree: {duree.seconds}s \\tbest_score: {mdl.best_score_:4.3} \\tbest_params: {mdl.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For a 1174 '1 and 0 balanced classes' sample:  \n",
    "  \n",
    ">> LinearSVC  \tduree: 1356s \tbest_score:  0.7 \tbest_params: {'C': 0.1, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l1'}  \n",
    ">>  \n",
    ">> XGBClassifier \tduree: 1889s \tbest_score: 0.72 \tbest_params: {'colsample_bytree': 1.0, 'gamma': 5, 'max_depth': 3, 'min_child_weight': 5, 'subsample': 0.8, 'verbosity': 0}  \n",
    ">>  \n",
    ">> GradBoostC \tduree: 1993s \tbest_score: 0.73 \tbest_params: {'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_split': 3, 'min_weight_fraction_leaf': 0.2, 'n_estimators': 200}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train, df_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Kfold Roc Curve and Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "tot_valid_y = np.zeros(y.shape[0])\n",
    "tot_valid_prob = np.zeros(y.shape[0])\n",
    "tot_score = []\n",
    "tot_feature_importances = []\n",
    "\n",
    "for splt, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    train_x, train_y = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    valid_x, valid_y = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # GradientBoostingClassifier\n",
    "    gbc = ensemble.GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        criterion='friedman_mse',\n",
    "        max_depth=3,\n",
    "        min_samples_split=3,\n",
    "        min_weight_fraction_leaf=0.2,\n",
    "    )\n",
    "    gbc.fit(train_x, train_y)\n",
    "\n",
    "    tot_valid_y[valid_idx] = valid_y\n",
    "    \n",
    "    valid_prob = gbc.predict_proba(valid_x)[:,1]\n",
    "    tot_valid_prob[valid_idx] = valid_prob\n",
    "    \n",
    "    tot_score.append(metrics.roc_auc_score(valid_y.values, valid_prob))\n",
    "    tot_feature_importances.append(gbc.feature_importances_)\n",
    "    \n",
    "tot_score = [round(1000*s)/1000 for s in tot_score] \n",
    "mean_score = sum(tot_score)/len(tot_score)\n",
    "print(f'tot_score   = {[s for s in tot_score]}')\n",
    "print(f'mean scores = {mean_score:5.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axe = plt.subplots(figsize=(8,8))\n",
    "[fpr, tpr, thr] = metrics.roc_curve(tot_valid_y,\n",
    "                                    tot_valid_prob,\n",
    "                                    pos_label=1)\n",
    "axe.plot(fpr, tpr, color='orange', lw=2)\n",
    "axe.set_title(f'Roc curve ({n_splits} splits) mean AUC = {mean_score:5.3}')\n",
    "axe.set_xlabel('Specificity')\n",
    "axe.set_ylabel('Sensitivity')\n",
    "axe.grid(visible=True, color='#eeeeee')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape features and importances to find features with main role in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_mean = pd.DataFrame(tot_feature_importances).mean().to_list()\n",
    "importance_std = pd.DataFrame(tot_feature_importances).std().to_list()\n",
    "df_features = pd.DataFrame(data=np.array([[c for c in X.columns], importance_mean, importance_std]).T,\n",
    "                           columns=['col name', 'mean def', 'std def'])\n",
    "df_features['mean def'] = df_features['mean def'].astype('float64')\n",
    "df_features['std def'] = df_features['std def'].astype('float64')\n",
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_draw = df_features.sort_values('mean def')\n",
    "df_draw = df_draw.iloc[-80:,:]\n",
    "fig, axes = plt.subplots(figsize=(14,int(df_draw.shape[0]//3.5)))\n",
    "axes.barh([x for x in range(df_draw.shape[0])],\n",
    "           df_draw['mean def'].values,\n",
    "           xerr = df_draw['std def'].values,\n",
    "           color = '#33aa33',\n",
    "           tick_label=df_draw['col name'].values)\n",
    "axes.set_title(f'Features Importance KFOLD - {df_draw.shape[0]} first features')\n",
    "axes.grid(visible=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = ensemble.GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        criterion='friedman_mse',\n",
    "        max_depth=3,\n",
    "        min_samples_split=3,\n",
    "        min_weight_fraction_leaf=0.2,\n",
    "    )\n",
    "gbc.fit(X, y)\n",
    "\n",
    "result = inspection.permutation_importance(gbc, X, y, n_repeats=8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_featpermut = pd.DataFrame(data=np.array([[c for c in X.columns], result.importances_mean, result.importances_std]).T,\n",
    "                   columns=['col permut', 'mean permut', 'std permut'])\n",
    "df_featpermut['mean permut'] = df_featpermut['mean permut'].astype('float64')\n",
    "df_featpermut['std permut'] = df_featpermut['std permut'].astype('float64')\n",
    "df_featpermut.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_draw = df_featpermut.sort_values('mean permut')\n",
    "df_draw = df_draw.iloc[-80:,:]\n",
    "fig, axes = plt.subplots(figsize=(14,int(df_draw.shape[0]//3.5)))\n",
    "axes.barh([x for x in range(df_draw.shape[0])],\n",
    "           df_draw['mean permut'].values,\n",
    "           xerr = df_draw['std permut'].values,\n",
    "           color = '#4444aa',\n",
    "           tick_label=df_draw['col permut'].values)\n",
    "axes.set_title(f'Features Importance PERMUTATION - {df_draw.shape[0]} first features')\n",
    "axes.grid(visible=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Combination Kfold and Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.concat([df_features, df_featpermut], axis=1)\n",
    "df_feat.drop('col permut', axis=1, inplace=True)\n",
    "\n",
    "del df_features, df_featpermut\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_draw = df_feat.sort_values('mean permut')\n",
    "df_draw = df_draw.iloc[-80:,:].sort_values('mean permut', ascending=False)\n",
    "max_val = max(df_draw['mean def'].max(), df_draw['mean permut'].max())\n",
    "ratio1 = 100/df_draw['mean def'].max()\n",
    "ratio2 = 100/df_draw['mean permut'].max()\n",
    "barWidth=0.8\n",
    "plt.figure(figsize=(14,int(len(df_draw)/3.5)))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.barh(df_draw['col name'].str[:40], df_draw['mean def']*ratio1,\n",
    "         left=100-(df_draw['mean def']*ratio1), color='#33aa33')\n",
    "plt.barh(df_draw['col name'].str[:40], df_draw['mean permut']*ratio2,\n",
    "         left=100, color ='#4444aa')\n",
    "plt.title('        Kfold mean  -   Permutation mean', fontsize=20)\n",
    "plt.yticks(fontsize=9, color='#222222')\n",
    "plt.xlabel('Importance')\n",
    "label_max_left = df_draw['mean def'].max()\n",
    "label_max_right = df_draw['mean permut'].max()\n",
    "labels = [\n",
    "          f'{label_max_left:4.2}', f'{label_max_left*0.75:4.2}',\n",
    "          f'{label_max_left*0.5:4.2}', f'{label_max_left*0.25:4.2}',\n",
    "          '0',\n",
    "          f'{label_max_right*0.25:4.2}', f'{label_max_right*0.5:4.2}',\n",
    "          f'{label_max_right*0.75:4.2}', f'{label_max_right:4.2}',\n",
    "          ]\n",
    "plt.xticks(np.arange(0,225, step=25), labels)\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['mean importance'] = (df_feat['mean def'] + df_feat['mean permut']) / 2\n",
    "df_feat['std importance'] = (df_feat['std def'] + df_feat['std permut']) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_draw = df_feat.sort_values('mean importance')\n",
    "df_draw = df_draw.iloc[-80:,:]\n",
    "fig, axes = plt.subplots(figsize=(14,int(df_draw.shape[0]//3.5)))\n",
    "axes.barh([x for x in range(df_draw.shape[0])],\n",
    "           df_draw['mean importance'].values,\n",
    "           xerr = df_draw['std importance'].values,\n",
    "           color = '#33aa33',\n",
    "           tick_label=df_draw['col name'].values)\n",
    "axes.set_title(f'Mean Features Importance KFOLD & Permut. - {df_draw.shape[0]} first features')\n",
    "axes.grid(visible=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Save File with Feature Importance, Description, Min, Max & Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df_feat.drop(columns=['mean def', 'std def', 'mean permut', 'std permut'])\\\n",
    "                 .sort_values('mean importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_descr = pd.read_csv(DIRSOURCE+'HomeCredit_columns_description.csv',\n",
    "                             encoding='Latin-1')\n",
    "\n",
    "def get_col_HC_description(c):\n",
    "    i = 0\n",
    "    colname = c\n",
    "    descr = '(Application) '\n",
    "    if c.startswith('BURO_'):\n",
    "        colname = c[5:]\n",
    "        descr = '(Bureau) '\n",
    "    if c.startswith('PREV_'):\n",
    "        colname = c[5:]\n",
    "        descr = '(Previous Application) '\n",
    "    if c.startswith('APPROVED_'):\n",
    "        colname = c[9:]\n",
    "        descr = '(Previous Application) '\n",
    "    if c.startswith('REFUSED_'):\n",
    "        colname = c[8:]\n",
    "        descr = '(Previous Application) '\n",
    "    if c.startswith('POS_'):\n",
    "        colname = c[4:]\n",
    "        descr = '(POS CASH Balance) '\n",
    "    if c.startswith('INSTAL_'):\n",
    "        colname = c[7:]\n",
    "        descr = '(Installments Payments) '\n",
    "    if c.startswith('CC_'):\n",
    "        colname = c[3:]\n",
    "        descr = '(Credit Card Balance) '\n",
    "    while i < df_descr.shape[0]:\n",
    "        if colname.startswith(str(df_descr.iloc[i,:]['Row'])):\n",
    "            if colname.endswith('_MEAN'):\n",
    "                descr = descr + 'MEAN, '\n",
    "            if colname.endswith('_MAX'):\n",
    "                descr = descr + 'MAX, '\n",
    "            if colname.endswith('_MIN'):\n",
    "                descr = descr + 'MIN, '\n",
    "            if colname.endswith('_SUM'):\n",
    "                descr = descr + 'SUM, '\n",
    "            descr = descr + str(df_descr.iloc[i,:]['Description']).replace('\\t', ' ')\n",
    "            break\n",
    "        i = i + 1\n",
    "    return descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat['description'] = df_feat['col name'].map(get_col_HC_description)\n",
    "\n",
    "del df_descr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMROWS = 1000000    # 1000000 to get complete dateset\n",
    "# File names with NUMROWS lines and Fill nan with zeros\n",
    "FILELOG_FNAN0 = DIRDATASET+'Credit_Home_Junction_Log_Fnan0_'+str(NUMROWS)+'.csv'\n",
    "\n",
    "df = pd.read_csv(FILELOG_FNAN0, encoding='Latin-1', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_min(c):\n",
    "    return df[c].min()\n",
    "\n",
    "def calc_max(c):\n",
    "    return df[c].max()\n",
    "\n",
    "def calc_mean(c):\n",
    "    return df[c].mean()\n",
    "\n",
    "df_feat['min val'] = df_feat['col name'].map(calc_min)\n",
    "df_feat['max val'] = df_feat['col name'].map(calc_max)\n",
    "df_feat['mean val'] = df_feat['col name'].map(calc_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.to_csv(DIRDATASET+'Credit_Home_Features.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "438838f77353e196c1617684cb98b864bf1da2ed28cf8470ba0df88c56da92f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
